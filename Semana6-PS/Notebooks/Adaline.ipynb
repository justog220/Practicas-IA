{"cells":[{"cell_type":"markdown","id":"c81baf78-6ef0-4e0a-b7e4-944ec07b3e7f","metadata":{"id":"c81baf78-6ef0-4e0a-b7e4-944ec07b3e7f"},"source":["# ADAptive LInear NEuron (ADALINE)"]},{"cell_type":"markdown","id":"49ec57b0-8e68-434b-a0e6-2ffa0c5cf2e4","metadata":{"id":"49ec57b0-8e68-434b-a0e6-2ffa0c5cf2e4"},"source":["### ¿Por qué son diferentes el perceptrón y el adaline?\n","<center><img src=\"https://drive.google.com/uc?export=view&id=1cqn7bXOEfDm7apLiNLzdUauMuaEW28Yb\" width=\"880\" alt=\"centered image\"></center>\n","De la figura se puede ver que en el algoritmo del Adaline se comparan las etiquetas de las clases esperadas con el valor (continuo o número real) de salida de la función de activación $\\phi$, de esta diferencia se calcula el error para actualizar los pesos. \n","En el perceptrón, por el contrario, se comparan las etiquetas de las clases esperadas con las etiquetas obtenidas en la predicción."]},{"cell_type":"markdown","id":"55c104e9-7738-4dad-81c8-6bd312d8c0a1","metadata":{"id":"55c104e9-7738-4dad-81c8-6bd312d8c0a1"},"source":["### ¿Qué es la función de costo $J(w)$?\n","Es la función que se busca minimizar durante el proceso de aprendizaje o entrenamiento.\n","En el caso del Adaline, podemos definirla como la _suma de los errores cuadráticos_ entre la salida calculada (con la fn. de activación $\\phi(y)$) y la etiqueta de clase esperada $y_d$. \n","$$J(w) = \\frac{1}{2}\\sum\\limits_{i=1}^N(y_d^{(i)} - \\phi(y^{(i)}))^2$$\n","Para el Adaline vamos a utilizar un algoritmo de optimización llamado _Descenso del Gradiente_.\n","\n","En este caso, la actualización de los pesos se hace de la siguiente manera: $$\\vec{w} = \\vec{w} + \\Delta\\vec{w}$$\n","Donde $\\Delta{w}$ queda definido como el gradiente negativo de la fn. de costo multiplicado por el factor de entrenamiento $$\\Delta{\\vec{w}} = - \\mu\\nabla{J}(\\vec{w})$$\n"]},{"cell_type":"markdown","id":"4e1028da-1672-449b-82f6-10806d9a8c80","metadata":{"id":"4e1028da-1672-449b-82f6-10806d9a8c80"},"source":["### Pero...¿Cómo calculamos el gradiente de la fn. de costo?\n","Una propiedad de nuestra función de costo es que es derivable, a diferencia de la fn. signo del perceptrón. Por tanto sólo debemos calcular la derivada parcial de la función de costo respecto de cada peso $w_j$ $$J(w) = \\frac{1}{2}\\sum\\limits_{i=1}^N(y_d^{(i)} - \\phi(y^{(i)}))^2$$ \n","\n","$$\\frac{\\partial{J}}{\\partial{w_j}} = - \\sum\\limits_{i}(y^{(i)} - \\phi(z^{(i)}))x^{(i)}_j$$ y la actualización de los pesos:\n","$$\\Delta{w_j} = -\\mu\\frac{\\partial{J}}{\\partial{w_j}}=\\mu\\sum\\limits_{i}(y^{(i)} - \\phi(z^{(i)}))x^{(i)}_j$$"]},{"cell_type":"markdown","id":"f3f15c34-b856-41b8-bab1-4b74c2d20fc3","metadata":{"id":"f3f15c34-b856-41b8-bab1-4b74c2d20fc3"},"source":["> La función de costo se calcula teniendo en cuenta todos los patrones de entrada (note el índice i en la ecuación de $J(w)$) por lo tanto la actualización de los pesos en el algoritmo del Adaline se calcula en base a todas las entradas (a diferencia de la actualización incremental de los pesos con cada patrón de entrada en el perceptrón). A esto se le llama también _Descenso del gradiente en lotes_ (batch gradient descent)"]},{"cell_type":"code","execution_count":null,"id":"a2cdd51e","metadata":{"id":"a2cdd51e"},"outputs":[],"source":["import numpy as np\n","from numpy.random import RandomState\n","import matplotlib.pyplot as plt\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"id":"9af67c58","metadata":{"id":"9af67c58"},"outputs":[],"source":["class Adaline(object):\n","    \"\"\"ADAptive LInear NEuron classifier.\n","    Parametros\n","    ------------\n","    lr : float\n","        factor de entrenamiento (entre 0.0 y 1.0)\n","    n_iter : int\n","        iteraciones para el entrenamiento.\n","    random_state : int\n","        Semilla generadora de números aleatorios para\n","        la inicialización de los pesos.\n","    \n","    Atributos\n","    -----------\n","    w_ : 1d-array\n","        Pesos despues de su ajuste.\n","    costo_ : list\n","        Función de costo : suma de los errores cuadráticos en cada época\n","    \"\"\"\n","    def __init__(self, lr=0.01, n_iter=50, random_state=1):\n","        self.lr = lr\n","        self.n_iter = n_iter\n","        self.random_state = random_state\n","\n","    def fit(self, X, y):\n","        \"\"\"Ajuste de los datos de entrenamiento.\n","        \n","        Parameters\n","        ----------\n","        X : {array-like}, shape = [n_muestras, n_caracteristicas]\n","          vector de entrenamiento\n","        y : array-like, shape = [n_muestras]\n","          vector target.\n","          \n","        Returns\n","        -------\n","        self : objeto\n","\n","        \"\"\"\n","        rgen = RandomState(self.random_state)\n","        #inicializo los pesos con valores aleatorios entre 0 y 1\n","        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n","        self.costo_ = []\n","\n","        for _ in range(self.n_iter):\n","            entrada = self.calcular_entrada(X)\n","            salida = self.fn_activacion(entrada)\n","            # El error se calcula como la diferencia entre las salidas esperadas y\n","            # la salida de la fn. de activación\n","            errores = (y - salida)              \n","            # --------actualización del vector de pesos--------------------------------\n","            self.w_[1:] += self.lr * X.T.dot(errores)\n","            self.w_[0] += self.lr * errores.sum()\n","            # función de costo\n","            costo = (errores**2).sum() / 2.0\n","            #---------------------------------------------------------------------------\n","            self.costo_.append(costo)\n","        return self\n","    \n","    def calcular_entrada(self, X):\n","        \"\"\"Cálculo de la entrada al Adaline\"\"\"\n","        # -------suma de los productos de los valores de entrada y los pesos -----------        \n","        return np.dot(X, self.w_[1:]) + self.w_[0]\n","        #-------------------------------------------------------------------------------\n","    \n","    def fn_activacion(self, X): \n","        \"\"\"Aplica la fn. de activación lineal\"\"\"\n","        #Se puede ver que la fn.  de activación es la fn. identidad, vamos a ver que en el\n","        #caso de la regresión logística (más adelante), usamos una fn. sigmoidea\n","        return X\n","\n","    def predict(self, X):\n","        \"\"\"devuelve la etiqueta de la clase pertenciente después de aplicar la fn. de activación\"\"\"\n","        return np.where(self.calcular_entrada(X) >= 0.0, 1, -1)"]},{"cell_type":"code","execution_count":null,"id":"57b3a826-e32d-4574-a360-ecb4c77738bf","metadata":{"id":"57b3a826-e32d-4574-a360-ecb4c77738bf"},"outputs":[],"source":["#Cargo el Dataset\n","df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n","y = df.iloc[0:100, 4].values\n","#convertimos las etiquetas de clases en  0 (Iris-setosa)  y 1 (Iris-versicolor)\n","y = np.where(y == 'Iris-setosa', -1, 1)\n","# columna 0 : longitud de sépalo, columna 2 : longitud de pétalo\n","X = df.iloc[0:100, [0, 2]].values\n","print(X.shape) #(100,2)\n","print(y.shape) #(100,)"]},{"cell_type":"code","execution_count":null,"id":"77eeef91-bc72-419e-acb8-6ec2f27084d3","metadata":{"id":"77eeef91-bc72-419e-acb8-6ec2f27084d3"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","sc.fit(X)                     \n","X_std = sc.transform(X)"]},{"cell_type":"markdown","id":"ad7b1e88-c355-457a-a7e4-4e8dd833a32d","metadata":{"id":"ad7b1e88-c355-457a-a7e4-4e8dd833a32d"},"source":["En la práctica, seleccionar el coeficiente de aprendizaje ($\\mu$) para la convergencia del algoritmo requiere algo de experimentación. El valor de $\\mu$ que seleccionemos hará la diferencia entre los dos casos que tenemos a continuación\n","<center><img src=\"https://drive.google.com/uc?export=view&id=1c-uve0PzNx9GQZvbBkxYf9arKatB0Rs5\" width= 700 alt=\"centered image\"></center>\n","\n","Como podemos ver en la gráfica a la derecha, un valor de $\\mu$ muy grande hará que el algoritmo del descenso del gradiente no converja y por lo tanto no se minimiza la función de costo $J(w)$. En la gráfica de la izquierda, un valor de $\\mu$ muy chico hará que el algoritmo converja pero necesitará de un gran número de épocas para la convergencia."]},{"cell_type":"markdown","id":"9da7667f-2167-46fd-a186-3247d89bbd8c","metadata":{"id":"9da7667f-2167-46fd-a186-3247d89bbd8c"},"source":["### Vamos a crear dos instancias de la Clase Adaline, uno con lr = 0.1 y el otro con lr= 0.01 y grafiquemos sus funciones de costo!!! "]},{"cell_type":"code","execution_count":null,"id":"1a2a9d13-87af-48b7-8893-5a53af32c125","metadata":{"id":"1a2a9d13-87af-48b7-8893-5a53af32c125"},"outputs":[],"source":["ada1 = Adaline(n_iter=30, lr=0.1, random_state=1)\n","ada1.fit(X_std, y)\n","ada2 = Adaline(n_iter=30, lr=0.01, random_state=1)\n","ada2.fit(X_std, y)\n","fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n","ax[0].plot( range(1, len(ada1.costo_) + 1), np.log10(ada1.costo_), marker='o')\n","ax[0].set_xlabel('Épocas')\n","ax[0].set_ylabel('log(Suma-error-cuadrático)')\n","ax[0].set_title('Adaline  lr = 0.1')\n","\n","ax[1].plot( range(1, len(ada2.costo_) + 1), np.log10(ada2.costo_), marker='o')\n","ax[1].set_xlabel('Épocas')\n","ax[1].set_ylabel('Suma-error-cuadrático')\n","_ = ax[1].set_title('Adaline  lr = 0.01')"]},{"cell_type":"markdown","id":"c3c61c9e-9ee3-4858-8472-453862fcfd26","metadata":{"tags":[],"id":"c3c61c9e-9ee3-4858-8472-453862fcfd26"},"source":["El algoritmo del adaline ilustra los conceptos claves de la definición y minimización de las funciones de costo continuas. Esto brinda las bases para el entendimiento de algoritmos de machine learning para clasificación más avanzados, como la regresión logística, máquinas de soporte vectorial y modelos de regresión que veremos más adelante en la asignatura."]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from matplotlib.colors import ListedColormap\n","\n","def plot_decision_regions(X, y, clasificador, test_idx=None, resolution=0.02):\n","    \n","    # setup marker generator and color map\n","    markers = ('s', 'o', '^', 'v', 'x')\n","    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n","    cmap = ListedColormap(colors[:len(np.unique(y))])\n","\n","    # plot the decision surface\n","    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n","    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n","\n","    \n","    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n","                           np.arange(x2_min, x2_max, resolution))\n","    \n","    Z = clasificador.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n","\n","    Z = Z.reshape(xx1.shape)\n","\n","    plt.contourf(xx1, xx2, Z, alpha=0.3, cmap=cmap)\n","    plt.xlim(xx1.min(), xx1.max())\n","    plt.ylim(xx2.min(), xx2.max())\n","\n","    for idx, cl in enumerate(np.unique(y)):\n","        plt.scatter(x=X[y == cl, 0], \n","                    y=X[y == cl, 1],\n","                    alpha=0.8, \n","                    c=colors[idx],\n","                    marker=markers[idx], \n","                    label=cl, \n","                    edgecolor='black')\n","\n","    # highlight test samples\n","    if test_idx:\n","        # plot all samples\n","        X_test, y_test = X[test_idx, :], y[test_idx]\n","\n","        plt.scatter(X_test[:, 0],\n","                    X_test[:, 1],\n","                    c=\"None\",\n","                    edgecolor='black',\n","                    alpha=1,\n","                    linewidth=1,\n","                    marker='o',\n","                    s=100, \n","                    label='test set')"],"metadata":{"id":"OgoOhmL-Om8Y"},"id":"OgoOhmL-Om8Y","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"8f6c9108-a9ae-48b3-abc5-1e1e06c33c0e","metadata":{"id":"8f6c9108-a9ae-48b3-abc5-1e1e06c33c0e"},"outputs":[],"source":["plt.figure(figsize=(10,7))\n","plot_decision_regions(X_std,y, ada2)\n","plt.xlabel('longitud de pétalo')\n","plt.ylabel('ancho de pétalo ')\n","#plt.xlim([4, 7.2])\n","plt.legend(loc='upper left')\n","plt.tight_layout()\n","plt.show()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}