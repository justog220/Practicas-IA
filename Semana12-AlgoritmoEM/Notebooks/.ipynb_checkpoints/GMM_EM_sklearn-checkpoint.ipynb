{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1jZnYw9q35xy41B1Fazu6IkWrYfw-opEh","authorship_tag":"ABX9TyPWt2sO/HELfoLfqk+Q8VDE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# GaussianMixture"],"metadata":{"id":"QTCGDcOq4ahg"}},{"cell_type":"markdown","source":["- Un modelo de mezcla de gaussianas (GMM) es un modelo probabilístico que supone que las instancias se generaron a partir de una mezcla de varias distribuciones gaussianas cuyos parámetros se desconocen.\n","\n","- Todas las instancias generadas a partir de una única distribución gaussiana forman un cluster que suele parecerse a un elipsoide.\n","\n","- Cuando se observa una instancia, se sabe que se ha generado a partir de una de las distribuciones gaussianas, pero no se sabe cuál de ellas ni cuáles son sus parámetros.\n","\n","- En la variante más sencilla de mezclas gaussianas, se debe conocer de antemano el número *k* de distribuciones gaussianas. Se supone que el conjunto de datos X se ha generado mediante el siguiente proceso probabilístico:\n","\n","- Para cada instancia, se elige aleatoriamente un cluster de entre *k* clusters. La probabilidad de elegir el k-ésimo cluster viene definida por el peso del cluster $\\pi_k$. El índice del cluster elegido para la i-ésima instancia se denomina $z_i$.\n","\n","- Si $z_i=k$, lo que significa que la i-ésima instancia ha sido asignada al *k*-ésimo clúster, la ubicación $x_i$ de esta instancia se muestrea aleatoriamente a partir de la distribución gaussiana con media $μ_k$ y matriz de covarianza $Σ_k$. Esto se denota como  $\\mathcal{N}(x_i| \\mu_k, \\Sigma_k)$.\n","\n","El algoritmo que se usa para estimar los pesos, media y matrices de covarianza de los clusters es el algoritmo de Esperanza-maximización."],"metadata":{"id":"tvea6Ntq844w"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"KATdnayccQcW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/Inteligencia Artificial/IA - Clases de Práctica/PRACTICA_2023/Semana12-AlgoritmoEM/Notebooks'"],"metadata":{"id":"yU42L8a9cZXZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.datasets import make_blobs\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.mixture import GaussianMixture"],"metadata":{"id":"KHgRnw5f-SBp"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QFgnFGQV8Q1l"},"outputs":[],"source":["X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n","X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n","X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n","X2 = X2 + [6, -8]\n","X = np.r_[X1, X2] #vstack\n","y = np.r_[y1, y2]"]},{"cell_type":"code","source":["def plot_clusters(X, y=None):\n","    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n","    plt.xlabel(\"$x_1$\", fontsize=14)\n","    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"],"metadata":{"id":"8s30sQTkCuYw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8, 4))\n","plot_clusters(X)\n","plt.show()"],"metadata":{"id":"o9_zCoHbCuzR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gm = GaussianMixture(n_components=3, n_init=10, random_state=42)\n","gm.fit(X)"],"metadata":{"id":"GmpPQRsECwUr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gm.weights_"],"metadata":{"id":"tiQMB1QqFyGv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gm.means_"],"metadata":{"id":"12qAsH1LF0rX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gm.covariances_"],"metadata":{"id":"u11HigM8F8F_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Converge el algoritmo?**"],"metadata":{"id":"uKUFM8OgGwUs"}},{"cell_type":"code","source":["gm.converged_"],"metadata":{"id":"_ldz36RpF9kQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Cuántas iteraciones realiza?**"],"metadata":{"id":"b78r6e4VG5Mk"}},{"cell_type":"code","source":["gm.n_iter_"],"metadata":{"id":"s6FvxgDuG9um"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#hard cluatering\n","gm.predict(X)"],"metadata":{"id":"DN4Is04JG_fR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#soft clustering\n","gm.predict_proba(X).round(3)"],"metadata":{"id":"RrLfOD5_HHf9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Tenemos un modelo generativo, por tanto, podemos muestrear instancias nuevas de nuestro modelo y obtener sus etiquetas**"],"metadata":{"id":"rkeToGiTHPN2"}},{"cell_type":"code","source":["X_new, y_new = gm.sample(6)\n","X_new"],"metadata":{"id":"qpMF6xYOHMuh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_new"],"metadata":{"id":"n00doBNyHebf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**también se puede obtener el log de la función de densidad de probabilidad (pdf)**\n","\n","Cuanto más grande el valor, mayor densidad de probabilidad"],"metadata":{"id":"EZtM8rNsHqYc"}},{"cell_type":"code","source":["gm.score_samples(X).round(2)"],"metadata":{"id":"EtL3R1fMHgPu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Si se calcula la exponencial a estos resultados, se obtiene el valor de la PDF en la ubicación de las instancias dadas. No se obtienen probabilidades, sino densidades de probabilidad: pueden tomar cualquier valor positivo, no sólo valores entre 0 y 1. Para estimar la probabilidad de que una instancia se encuentre dentro de una región concreta, habría que integrar la PDF sobre esa región (si se hace sobre todo el espacio de posibles ubicaciones de las instancias, el resultado será 1)."],"metadata":{"id":"RReufwdUJ3wt"}},{"cell_type":"code","source":["from utiles import plot_gaussian_mixture\n","\n","plt.figure(figsize=(12, 7))\n","plot_gaussian_mixture(gm, X)\n","plt.show()"],"metadata":{"id":"zhH8bOH4H6JY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**La gráfica muestra las medias de los clusters, los límites de decisión (líneas rojas) y las curvas de contornos de la densidad de puntos.**\n","\n","En el ejemplo anterior, hemos facilitado la tarea generando los datos con un conjunto de distribuciones gaussianas en 2D. También, le dimos al algoritmo el número correcto de clusters. Cuando se tienen muchas dimensiones, o muchos clusters, o pocas instancias, el algoritmo EM puede tener dificultades para converger a la solución óptima. Puede ser necesario reducir la dificultad de la tarea limitando el número de parámetros que el algoritmo tiene que aprender. Una forma de hacerlo es limitar las formas y orientaciones que pueden tener los clusters. Esto se consigue imponiendo restricciones a las matrices de covarianza.\n","\n","Se puede establecer el parámetro `covariance_type` en uno de los siguientes valores:\n","\n","**\"spherical\"**\n","\n","Todos los clusters son esféricos pero pueden tener diámetros diferentes.\n","\n","**\"diag\"**\n","\n","Los clusters pueden tener forma cualquier forma elipsoidal y de cualquier tamaño pero sus ejes deben ser paralelos a los ejes coordenados (las matrices de covarianza son diagonales).\n","\n","**\"tied\"**\n","\n","Todos los clusters deben tener la misma forma elipsoidal, tamaño y orientación (todos los clusters comparten igual matriz de covarianza).\n","\n","Por defecto, este parámetro es igual a **\"full\"\"**, es decir que cada cluster puede tomar cualquier forma, tamaño y orientación (tienen su propia matriz de covarianza sin restricciones)"],"metadata":{"id":"v9KPcw7Nc5vI"}},{"cell_type":"code","source":["gm_full = GaussianMixture(n_components=3, n_init=10, covariance_type=\"full\", random_state=42)\n","\n","gm_tied = GaussianMixture(n_components=3, n_init=10, covariance_type=\"tied\", random_state=42)\n","\n","gm_spherical = GaussianMixture(n_components=3, n_init=10, covariance_type=\"spherical\", random_state=42)\n","\n","gm_diag = GaussianMixture(n_components=3, n_init=10, covariance_type=\"diag\", random_state=42)\n","\n","gm_full.fit(X)\n","gm_tied.fit(X)\n","gm_spherical.fit(X)\n","gm_diag.fit(X)\n","\n","plt.figure(figsize=(15, 10))\n","\n","plt.subplot(221)\n","plot_gaussian_mixture(gm_tied, X)\n","plt.title(f'covariance_type=\"{gm_tied.covariance_type}\"')\n","\n","plt.subplot(222)\n","plot_gaussian_mixture(gm_spherical, X, show_ylabels=False)\n","plt.title(f'covariance_type=\"{gm_spherical.covariance_type}\"')\n","\n","plt.subplot(223)\n","plot_gaussian_mixture(gm_full, X)\n","plt.title(f'covariance_type=\"{gm_full.covariance_type}\"')\n","\n","plt.subplot(224)\n","plot_gaussian_mixture(gm_diag, X, show_ylabels=False)\n","plt.title(f'covariance_type=\"{gm_diag.covariance_type}\"')"],"metadata":{"id":"7kYAa0G2cj_L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Detección de anomalías usando GMM\n","\n","Utilizar un modelo de mezcla de gaussianas para la detección de anomalías es bastante sencillo: cualquier instancia situada en una región de baja densidad puede considerarse una anomalía. Se bebe definir qué umbral de densidad desea utilizar."],"metadata":{"id":"7pezFbjm72I_"}},{"cell_type":"code","source":["densities = gm.score_samples(X)\n","density_threshold = np.percentile(densities, 2)\n","print(density_threshold)\n","anomalies = X[densities < density_threshold]"],"metadata":{"id":"H3MwCuSy8Hve"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(8, 4))\n","\n","plot_gaussian_mixture(gm, X)\n","plt.scatter(anomalies[:, 0], anomalies[:, 1], color='r', marker='*')\n","\n","plt.show()"],"metadata":{"id":"l0grLIvq8t6Q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Selección del número de clusters\n","\n","No podemos utilizar la inercia o el coeficiente de silueta porque ambas suponen que los clusters son esféricos. En su lugar, podemos intentar encontrar el modelo que minimice un criterio de información teórico como el Criterio de Información Bayesiano (BIC) o el Criterio de Información de Akaike (AIC):\n","\n","${BIC} = {\\log(m)p - 2\\log({\\hat L})}$\n","\n","${AIC} = 2p - 2\\log(\\hat L)$\n","\n","* $m$ es el número de instancias.\n","* $p$ es el número de parámetros aprendidos por el modelo.\n","* $\\hat L$ es el valor máximo de la función de verosimilitud del modelo."],"metadata":{"id":"822sJbYf9FzT"}},{"cell_type":"code","source":["gm.bic(X)"],"metadata":{"id":"crw6MeLt8x4A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gm.aic(X)"],"metadata":{"id":"rf8Db446zfga"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gms_per_k = [GaussianMixture(n_components=k, n_init=10, random_state=42).fit(X) for k in range(1, 11)]\n","bics = [model.bic(X) for model in gms_per_k]\n","aics = [model.aic(X) for model in gms_per_k]\n","\n","plt.figure(figsize=(8, 3))\n","plt.plot(range(1, 11), bics, \"bo-\", label=\"BIC\")\n","plt.plot(range(1, 11), aics, \"go--\", label=\"AIC\")\n","plt.xlabel(\"$k$\")\n","plt.ylabel(\"Criterio de Información\")\n","plt.axis([1, 9.5, min(aics) - 50, max(aics) + 50])\n","plt.annotate(\"\", xy=(3, bics[2]), xytext=(3.4, 8650),\n","             arrowprops=dict(facecolor='black', shrink=0.1))\n","plt.text(3.5, 8660, \"Mínimo\", horizontalalignment=\"center\")\n","plt.legend()\n","plt.grid()\n","plt.show()"],"metadata":{"id":"flHFlnj7zpfK"},"execution_count":null,"outputs":[]}]}